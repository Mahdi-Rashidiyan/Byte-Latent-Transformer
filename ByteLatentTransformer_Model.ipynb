{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Byte Latent Transformer - Part 1: Model Architecture\n",
        "Introduction\n",
        "The Byte Latent Transformer is an advanced neural network architecture that processes data at the byte level. Unlike traditional transformers that work with word or subword tokens, this model operates directly on bytes, which offers several advantages:\n",
        "\n",
        "Language-agnostic: Works with any language without special tokenization\n",
        "Universal data handling: Can process text, code, and even binary data\n",
        "No out-of-vocabulary issues: Every possible byte is in the vocabulary\n",
        "\n",
        "In this implementation, we'll build a Byte Latent Transformer from scratch using PyTorch.\n",
        "Implementation Overview\n",
        "We'll implement the model in these parts:\n",
        "\n",
        "1.Model Architecture\n",
        "\n",
        "2.Training Pipeline\n",
        "\n",
        "3.Inference and Evaluation"
      ],
      "metadata": {
        "id": "bYS9ufFJYzB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Model** **Architecture**"
      ],
      "metadata": {
        "id": "RowvMmNwZpY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Required** **Libraries**"
      ],
      "metadata": {
        "id": "bOCqSYasZHGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FcigQWhMYsnq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Core Components**\n",
        "**1. Byte Embedding Layer**\n",
        "\n",
        "This layer converts input bytes (0-255) into embeddings:"
      ],
      "metadata": {
        "id": "zagzoBLyZOTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ByteEmbedding(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # 256 possible byte values (0-255)\n",
        "        self.embedding = nn.Embedding(256, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)"
      ],
      "metadata": {
        "id": "63KafwtJZFIC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Positional Encoding**\n",
        "\n",
        "For transformers to understand sequence order:bold text"
      ],
      "metadata": {
        "id": "Whz-Cav1Za6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim, max_seq_length=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_length, hidden_dim)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register buffer (persistent state)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to input embeddings\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "FHbhcQWHZFLC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Multi-Head Self-Attention**\n",
        "\n",
        "The core attention mechanism:"
      ],
      "metadata": {
        "id": "Ch2-rw-_Z8Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.q_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections and reshape for multi-head attention\n",
        "        q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply mask if provided (for causal attention)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax and dropout\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply output projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "rTF8fLLsZFN1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Feed-Forward Network**\n",
        "\n",
        "The position-wise feed-forward network:"
      ],
      "metadata": {
        "id": "0B172BSaaGIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first linear layer with GELU activation\n",
        "        x = F.gelu(self.linear1(x))\n",
        "        # Apply dropout and second linear layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5K-EY8QiaFF8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Encoder Layer**\n",
        "\n",
        "Combines attention and feed-forward networks:"
      ],
      "metadata": {
        "id": "y6nINifbaRfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(hidden_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer norm\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "U8hK9j0YaULq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Latent Projection Layer**\n",
        "\n",
        "This is what makes it a \"Latent\" transformer - projecting bytes to a latent space:"
      ],
      "metadata": {
        "id": "0gLUa_6oacCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentProjection(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.down_proj = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.up_proj = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.norm = nn.LayerNorm(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project to latent space\n",
        "        latent = self.down_proj(x)\n",
        "        latent = self.norm(latent)\n",
        "\n",
        "        # Project back to hidden space\n",
        "        return self.up_proj(latent)"
      ],
      "metadata": {
        "id": "2WOkCy0tadZx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Complete Byte Latent Transformer Model**\n",
        "\n",
        "Now we'll put everything together:"
      ],
      "metadata": {
        "id": "Wnh6DXkFaj26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ByteLatentTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_dim=512,\n",
        "                 latent_dim=256,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 ff_dim=2048,\n",
        "                 dropout=0.1,\n",
        "                 max_seq_length=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        # Byte embedding layer\n",
        "        self.byte_embedding = ByteEmbedding(hidden_dim)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_dim, max_seq_length)\n",
        "\n",
        "        # Latent projection\n",
        "        self.latent_projection = LatentProjection(hidden_dim, latent_dim)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(hidden_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(hidden_dim, 256)  # 256 possible byte values\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Convert input bytes to embeddings and add positional encoding\n",
        "        x = self.byte_embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply latent projection\n",
        "        x = self.latent_projection(x)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Project to output vocabulary\n",
        "        output = self.output_projection(x)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8OXVzQL5amaZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Creating a Causal Mask**\n",
        "\n",
        "For autoregressive generation, we need a causal mask:"
      ],
      "metadata": {
        "id": "JHho2b0Jau1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(size):\n",
        "    \"\"\"Create a causal mask for autoregressive generation.\"\"\"\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "    return ~mask  # Flip so 1s indicate allowed positions"
      ],
      "metadata": {
        "id": "A24UlfyXaxER"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Model**\n",
        "\n",
        "Let's test our model with a small example:"
      ],
      "metadata": {
        "id": "TMk50R5Va1vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    # Create model with small dimensions for testing\n",
        "    model = ByteLatentTransformer(\n",
        "        hidden_dim=64,\n",
        "        latent_dim=32,\n",
        "        num_layers=2,\n",
        "        num_heads=4,\n",
        "        ff_dim=128\n",
        "    )\n",
        "\n",
        "    # Create a sample input (batch_size=2, seq_length=10)\n",
        "    x = torch.randint(0, 256, (2, 10))\n",
        "\n",
        "    # Create causal mask\n",
        "    mask = create_causal_mask(10).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(x, mask)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"Model test successful!\")\n",
        "\n",
        "# Run the test\n",
        "test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvZ7V4qfa3nh",
        "outputId": "810dd60c-074e-4f25-f740-367701f5cce5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 10])\n",
            "Output shape: torch.Size([2, 10, 256])\n",
            "Model test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next part, we'll implement the training pipeline including data loading, optimization, and loss functions."
      ],
      "metadata": {
        "id": "E4m7tjA_a83C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Latent Transformer - Part 2: Training Pipeline"
      ],
      "metadata": {
        "id": "B-X92BLkf7GR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "In this second part, we'll implement the training pipeline for our Byte Latent Transformer. This includes:\n",
        "\n",
        "1.Data processing at the byte level\n",
        "\n",
        "2.Creating datasets and data loaders\n",
        "\n",
        "3.Setting up the training **loop** **bold text** **bold text**\n",
        "\n",
        "4.Implementing optimization and learning rate scheduling"
      ],
      "metadata": {
        "id": "xIRcOjJmgHHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Required Libraries**"
      ],
      "metadata": {
        "id": "p0xMFyY6gQ7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "hdGePyTagSnV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Processing**\n",
        "\n",
        "First, let's create utilities to convert text to byte sequences and vice versa:"
      ],
      "metadata": {
        "id": "efKVUizkgXLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bytes(text):\n",
        "    \"\"\"Convert text to a list of byte values.\"\"\"\n",
        "    return list(text.encode('utf-8'))\n",
        "\n",
        "def bytes_to_text(byte_list):\n",
        "    \"\"\"Convert a list of byte values back to text.\"\"\"\n",
        "    return bytes(byte_list).decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "OO0Ncm0EgYwl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Dataset Creation**\n",
        "\n",
        "Now, let's create a dataset for byte-level language modeling:"
      ],
      "metadata": {
        "id": "OgEemv4lgnpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ByteDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data  # Raw bytes\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # Minus 1 because each example needs a target (next byte)\n",
        "        return max(0, len(self.data) - self.seq_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get input sequence\n",
        "        input_seq = self.data[idx:idx+self.seq_length]\n",
        "\n",
        "        # Get target sequence (shifted by 1)\n",
        "        target_seq = self.data[idx+1:idx+self.seq_length+1]\n",
        "\n",
        "        # Convert to tensors\n",
        "        input_tensor = torch.tensor(input_seq, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor"
      ],
      "metadata": {
        "id": "nxttXzMdgpD9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function to load and preprocess text data:"
      ],
      "metadata": {
        "id": "GK_2Vv8jgtRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_data(file_path, seq_length):\n",
        "    \"\"\"Load text file and create a ByteDataset.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        # Try with a different encoding if UTF-8 fails\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            text = f.read()\n",
        "\n",
        "    # Convert text to bytes\n",
        "    byte_data = text_to_bytes(text)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = ByteDataset(byte_data, seq_length)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "-HDauY_jguKW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Training Utils**\n",
        "\n",
        "Let's implement some utility functions for tracking training progress:"
      ],
      "metadata": {
        "id": "eTcx5NFBgzBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingMonitor():\n",
        "    def __init__(self):\n",
        "        self.epochs = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def update(self, epoch, loss, val_loss=None):\n",
        "        self.epochs.append(epoch)\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        if val_loss is not None:\n",
        "            self.val_losses.append(val_loss)\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                return True  # Signal to save checkpoint\n",
        "        return False\n",
        "\n",
        "    def plot(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.epochs, self.losses, label='Training Loss')\n",
        "\n",
        "        if self.val_losses:\n",
        "            plt.plot(self.epochs, self.val_losses, label='Validation Loss')\n",
        "\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Training Progress')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "FON7erqvg0jF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Learning Rate Scheduler**\n",
        "\n",
        "Let's implement a learning rate scheduler with warmup and cosine decay:"
      ],
      "metadata": {
        "id": "aXEIBlkvg5hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupCosineScheduler():\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        self.min_lr_ratio = min_lr_ratio\n",
        "\n",
        "        # Get initial learning rate\n",
        "        self.base_lr = optimizer.param_groups[0]['lr']\n",
        "        self.min_lr = self.base_lr * min_lr_ratio\n",
        "\n",
        "        self.step_count = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "        lr = self.get_lr()\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Warmup phase\n",
        "        if self.step_count < self.warmup_steps:\n",
        "            return self.base_lr * (self.step_count / self.warmup_steps)\n",
        "\n",
        "        # Cosine decay phase\n",
        "        progress = (self.step_count - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        # Scale between base_lr and min_lr\n",
        "        return self.min_lr + (self.base_lr - self.min_lr) * cosine_decay"
      ],
      "metadata": {
        "id": "4zuUDubYg7MF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Training Function**\n",
        "\n",
        "Now, let's implement the full training loop:"
      ],
      "metadata": {
        "id": "BIAp5PG9g_8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader=None,\n",
        "                epochs=10, lr=3e-4, warmup_steps=1000, device='cuda',\n",
        "                save_dir='checkpoints', save_every=1):\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
        "\n",
        "    # Calculate total steps for scheduler\n",
        "    total_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    # Setup scheduler\n",
        "    scheduler = WarmupCosineScheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    # Setup training monitor\n",
        "    monitor = TrainingMonitor()\n",
        "\n",
        "    # Create a causal mask once for the maximum sequence length\n",
        "    seq_length = next(iter(train_dataloader))[0].size(1)\n",
        "    causal_mask = create_causal_mask(seq_length).to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, causal_mask)\n",
        "\n",
        "            # Reshape outputs for loss calculation\n",
        "            # [batch_size, seq_len, vocab_size] -> [batch_size * seq_len, vocab_size]\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            epoch_loss += loss.item()\n",
        "            avg_loss = epoch_loss / (batch_idx + 1)\n",
        "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\",\n",
        "                                    lr=f\"{optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Calculate average epoch loss\n",
        "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = None\n",
        "        if val_dataloader:\n",
        "            val_loss = validate_model(model, val_dataloader, causal_mask, device)\n",
        "            print(f\"Epoch {epoch}: train_loss={avg_epoch_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
        "                  f\"time={time.time() - start_time:.2f}s\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch}: train_loss={avg_epoch_loss:.4f}, \"\n",
        "                  f\"time={time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Update monitor and save checkpoint if it's the best model\n",
        "        save_checkpoint = monitor.update(epoch, avg_epoch_loss, val_loss)\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if save_checkpoint or (epoch % save_every == 0):\n",
        "            checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch}.pt\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_epoch_loss,\n",
        "                'val_loss': val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "            # Save best model separately\n",
        "            if save_checkpoint:\n",
        "                best_path = os.path.join(save_dir, \"best_model.pt\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'train_loss': avg_epoch_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                }, best_path)\n",
        "                print(f\"Best model saved with validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Plot training progress\n",
        "    monitor.plot()\n",
        "\n",
        "    return model, monitor"
      ],
      "metadata": {
        "id": "6fDwoXxKhBhU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Validation Function**\n",
        "\n",
        "Let's implement the validation function:"
      ],
      "metadata": {
        "id": "FBiWUMZahIz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_dataloader, mask, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_dataloader:\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, mask)\n",
        "\n",
        "            # Reshape outputs for loss calculation\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_dataloader)"
      ],
      "metadata": {
        "id": "RrfN44WQhKEk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Data Preparation**\n",
        "\n",
        "Now, let's write a function to prepare our data and create data loaders:"
      ],
      "metadata": {
        "id": "xvxwDvNqhQaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(file_path, seq_length=128, batch_size=32, val_split=0.1, num_workers=2):\n",
        "    \"\"\"Prepare data for training and validation.\"\"\"\n",
        "\n",
        "    # Load dataset\n",
        "    full_dataset = load_text_data(file_path, seq_length)\n",
        "\n",
        "    # Split into train and validation\n",
        "    val_size = int(len(full_dataset) * val_split)\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "HtP38TxOhRsc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Putting it All Together**\n",
        "\n",
        "Let's create a function to initialize and train our model:"
      ],
      "metadata": {
        "id": "qje5Y7nBhXaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(size):\n",
        "    \"\"\"Create a causal mask for autoregressive generation.\"\"\"\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "    return ~mask  # Flip so 1s indicate allowed positions\n",
        "\n",
        "def train_byte_latent_transformer(file_path,\n",
        "                                 hidden_dim=512,\n",
        "                                 latent_dim=256,\n",
        "                                 num_layers=6,\n",
        "                                 num_heads=8,\n",
        "                                 ff_dim=2048,\n",
        "                                 dropout=0.1,\n",
        "                                 max_seq_length=512,\n",
        "                                 batch_size=16,\n",
        "                                 epochs=10,\n",
        "                                 learning_rate=3e-4,\n",
        "                                 warmup_steps=1000,\n",
        "                                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Initialize and train a ByteLatentTransformer model.\"\"\"\n",
        "\n",
        "    # Create model\n",
        "    model = ByteLatentTransformer(\n",
        "        hidden_dim=hidden_dim,\n",
        "        latent_dim=latent_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        ff_dim=ff_dim,\n",
        "        dropout=dropout,\n",
        "        max_seq_length=max_seq_length\n",
        "    )\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader = prepare_data(\n",
        "        file_path,\n",
        "        seq_length=max_seq_length,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trained_model, monitor = train_model(\n",
        "        model=model,\n",
        "        train_dataloader=train_loader,\n",
        "        val_dataloader=val_loader,\n",
        "        epochs=epochs,\n",
        "        lr=learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return trained_model, monitor"
      ],
      "metadata": {
        "id": "vNtUQtq9hY7M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**\n",
        "\n",
        "Here's how you can use the training pipeline:"
      ],
      "metadata": {
        "id": "zTtqJm7phha2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "# Save the file\n",
        "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text[:50000])\n",
        "\n",
        "print(\"File downloaded and saved as shakespeare.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuwJOEt0mJsR",
        "outputId": "a3eb83fa-1ab1-423f-8724-9bdffc372de7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with a text file\n",
        "def main():\n",
        "    # Check if GPU is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and train the model\n",
        "    # For a quick test, use smaller dimensions\n",
        "    model, monitor = train_byte_latent_transformer(\n",
        "        file_path='/content/shakespeare.txt',  # Replace with your file\n",
        "        hidden_dim=256,  # Smaller for faster training\n",
        "        latent_dim=128,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        ff_dim=1024,\n",
        "        max_seq_length=128,\n",
        "        batch_size=8,\n",
        "        epochs=5\n",
        "    )\n",
        "\n",
        "    # Plot training progress\n",
        "    monitor.plot()\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dkpbfuRJhjBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of the speed of cpu I can't run it currently now"
      ],
      "metadata": {
        "id": "rzNPkmUrnkrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next part, we'll implement the inference and evaluation functionality to generate text with our trained model."
      ],
      "metadata": {
        "id": "yP1HZhfxiVBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Latent Transformer - Part 3: Inference and Evaluation\n",
        "In this final part, we'll implement the inference and evaluation functionality for our Byte Latent Transformer. This includes:\n",
        "\n",
        "1.Text generation with the trained model\n",
        "\n",
        "2.Model evaluation metrics\n",
        "\n",
        "3.Sample applications\n",
        "\n",
        "4.Saving and loading models\n",
        "\n",
        "5.Complete example with Google Colab integration\n"
      ],
      "metadata": {
        "id": "B5lTzO7gn0wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Required** **Libraries**"
      ],
      "metadata": {
        "id": "BFczpOhBoH5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "oEL4hVSPiWez"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Text Generation Functions**\n",
        "First, let's implement functions for generating text with our trained model:"
      ],
      "metadata": {
        "id": "_abu7NJxoQVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(size):\n",
        "    \"\"\"Create a causal mask for autoregressive generation.\"\"\"\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "    return ~mask  # Flip so 1s indicate allowed positions\n",
        "\n",
        "def sample_from_logits(logits, temperature=1.0, top_k=None, top_p=None):\n",
        "    \"\"\"Sample from logits with optional temperature, top-k, and nucleus sampling.\"\"\"\n",
        "    # Apply temperature\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Apply top-k filtering if specified\n",
        "    if top_k is not None:\n",
        "        # Keep only the top-k values, set the rest to -inf\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        threshold = v[:, -1].unsqueeze(1)\n",
        "        logits = torch.where(logits < threshold,\n",
        "                           torch.ones_like(logits) * float('-inf'),\n",
        "                           logits)\n",
        "\n",
        "    # Apply nucleus (top-p) sampling if specified\n",
        "    if top_p is not None:\n",
        "        # Sort logits in descending order\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "\n",
        "        # Calculate cumulative probabilities\n",
        "        probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cum_probs = torch.cumsum(probs, dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cum_probs > top_p\n",
        "\n",
        "        # Shift the indices to the right to keep the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create a mask for indices to remove\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "            1, sorted_indices, sorted_indices_to_remove\n",
        "        )\n",
        "\n",
        "        # Set logits to -inf where needed\n",
        "        logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Sample from the probability distribution\n",
        "    next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "    return next_token\n",
        "\n",
        "def generate_text(model, prompt_bytes, max_length=100, temperature=0.8,\n",
        "                  top_k=50, top_p=0.9, device='cuda'):\n",
        "    \"\"\"Generate text using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Convert prompt to tensor\n",
        "    if isinstance(prompt_bytes, str):\n",
        "        # If prompt is a string, convert to bytes\n",
        "        prompt_bytes = list(prompt_bytes.encode('utf-8'))\n",
        "\n",
        "    input_tensor = torch.tensor([prompt_bytes], dtype=torch.long).to(device)\n",
        "    generated_bytes = list(prompt_bytes)\n",
        "\n",
        "    # Generate text auto-regressively\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Use only the last 'max_seq_length' tokens if input is too long\n",
        "            if hasattr(model, 'positional_encoding') and hasattr(model.positional_encoding, 'pe'):\n",
        "                max_seq_length = model.positional_encoding.pe.size(1)\n",
        "                if input_tensor.size(1) > max_seq_length:\n",
        "                    input_tensor = input_tensor[:, -max_seq_length:]\n",
        "\n",
        "            # Create causal mask for the sequence\n",
        "            causal_mask = create_causal_mask(input_tensor.size(1)).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(input_tensor, causal_mask)\n",
        "\n",
        "            # Get predictions for the next token\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = sample_from_logits(\n",
        "                next_token_logits.unsqueeze(0),\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p\n",
        "            )\n",
        "\n",
        "            # Add the generated token to the input for the next iteration\n",
        "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # Store the generated byte\n",
        "            generated_bytes.append(next_token.item())\n",
        "\n",
        "            # Stop if we generate the end of text token (if you have one)\n",
        "            # For now, we'll just generate up to max_length\n",
        "\n",
        "    # Convert bytes back to text\n",
        "    try:\n",
        "        generated_text = bytes(generated_bytes).decode('utf-8', errors='replace')\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding bytes: {e}\")\n",
        "        generated_text = \"Error decoding generated bytes\"\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "eRvSfQU5oRu3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model Evaluation Metrics**\n",
        "Let's implement functions to evaluate our model:"
      ],
      "metadata": {
        "id": "42aIU6YmoXPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_perplexity(model, dataloader, device='cuda'):\n",
        "    \"\"\"Evaluate model perplexity on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(dataloader, desc=\"Evaluating perplexity\"):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Create causal mask\n",
        "            causal_mask = create_causal_mask(inputs.size(1)).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, causal_mask)\n",
        "\n",
        "            # Reshape outputs for loss calculation\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
        "\n",
        "            # Update counters\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.numel()\n",
        "\n",
        "    # Calculate perplexity\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "    return perplexity.item()\n",
        "\n",
        "def evaluate_model_speed(model, seq_length=128, batch_size=1, num_iterations=10, device='cuda'):\n",
        "    \"\"\"Evaluate model inference speed.\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create random input tensors\n",
        "    inputs = torch.randint(0, 256, (batch_size, seq_length), device=device)\n",
        "\n",
        "    # Create causal mask\n",
        "    causal_mask = create_causal_mask(seq_length).to(device)\n",
        "\n",
        "    # Warm up\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            _ = model(inputs, causal_mask)\n",
        "\n",
        "    # Measure inference time\n",
        "    torch.cuda.synchronize() if device == 'cuda' else None\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(inputs, causal_mask)\n",
        "\n",
        "    torch.cuda.synchronize() if device == 'cuda' else None\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / num_iterations\n",
        "    tokens_per_second = (batch_size * seq_length) / avg_time\n",
        "\n",
        "    return {\n",
        "        'total_time': total_time,\n",
        "        'avg_time_per_batch': avg_time,\n",
        "        'tokens_per_second': tokens_per_second\n",
        "    }"
      ],
      "metadata": {
        "id": "ZUkvyMrFoYf_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Save and Load Model** **Functions**"
      ],
      "metadata": {
        "id": "LlxT_NJ4obDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer=None, epoch=None, loss=None, path='model.pt'):\n",
        "    \"\"\"Save model and training state.\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "    }\n",
        "\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    if epoch is not None:\n",
        "        checkpoint['epoch'] = epoch\n",
        "\n",
        "    if loss is not None:\n",
        "        checkpoint['loss'] = loss\n",
        "\n",
        "    # Add model hyperparameters for easy loading\n",
        "    checkpoint['model_config'] = {\n",
        "        'hidden_dim': model.byte_embedding.embedding.weight.size(1),\n",
        "        'latent_dim': model.latent_projection.down_proj.out_features,\n",
        "        'num_layers': len(model.encoder_layers),\n",
        "        'num_heads': model.encoder_layers[0].self_attn.num_heads,\n",
        "        'ff_dim': model.encoder_layers[0].feed_forward.linear1.out_features,\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path, device='cuda'):\n",
        "    \"\"\"Load model from checkpoint.\"\"\"\n",
        "\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "    # Get model configuration\n",
        "    config = checkpoint.get('model_config', {})\n",
        "\n",
        "    # Create model with the same configuration\n",
        "    model = ByteLatentTransformer(\n",
        "        hidden_dim=config.get('hidden_dim', 512),\n",
        "        latent_dim=config.get('latent_dim', 256),\n",
        "        num_layers=config.get('num_layers', 6),\n",
        "        num_heads=config.get('num_heads', 8),\n",
        "        ff_dim=config.get('ff_dim', 2048)\n",
        "    )\n",
        "\n",
        "    # Load the state dict\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model, checkpoint"
      ],
      "metadata": {
        "id": "K1GZHT76ofOn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Sample Applications**\n",
        "\n",
        "**4.1 Text Completion **"
      ],
      "metadata": {
        "id": "jexP2Dp3oluR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(model, prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.9, device='cuda'):\n",
        "    \"\"\"Complete text given a prompt.\"\"\"\n",
        "    return generate_text(\n",
        "        model=model,\n",
        "        prompt_bytes=prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "uoIaJVUgor4_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Interactive Text Generation"
      ],
      "metadata": {
        "id": "-PLp7Gqyozhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_generation(model, device='cuda'):\n",
        "    \"\"\"Interactive text generation with the model.\"\"\"\n",
        "    print(\"=== Interactive Text Generation ===\")\n",
        "    print(\"Type your prompt and press Enter. Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"\\nPrompt: \")\n",
        "        if prompt.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        print(\"\\nGenerating...\")\n",
        "\n",
        "        try:\n",
        "            temperature = float(input(\"Temperature (0.1-1.5, default 0.8): \") or 0.8)\n",
        "            max_length = int(input(\"Max length to generate (default 100): \") or 100)\n",
        "\n",
        "            generated_text = complete_text(\n",
        "                model=model,\n",
        "                prompt=prompt,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            print(\"\\n=== Generated Text ===\")\n",
        "            print(generated_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "nI36T6uno1bm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3 Byte-Level Analysis"
      ],
      "metadata": {
        "id": "dgKPmv5So6VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_byte_distribution(model, text, device='cuda'):\n",
        "    \"\"\"Analyze byte distribution predictions for a given text.\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Convert text to bytes\n",
        "    bytes_data = list(text.encode('utf-8'))\n",
        "    input_tensor = torch.tensor([bytes_data], dtype=torch.long).to(device)\n",
        "\n",
        "    # Create causal mask\n",
        "    causal_mask = create_causal_mask(input_tensor.size(1)).to(device)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor, causal_mask)\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get top predicted bytes for each position\n",
        "    top_k = 5\n",
        "    top_probs, top_indices = torch.topk(probs[0], k=top_k, dim=-1)\n",
        "\n",
        "    # Print analysis\n",
        "    print(f\"Byte-level analysis for: '{text}'\")\n",
        "    print(\"Format: position -> actual byte -> top predictions\")\n",
        "\n",
        "    for i in range(len(bytes_data)):\n",
        "        actual_byte = bytes_data[i]\n",
        "        byte_char = chr(actual_byte) if 32 <= actual_byte <= 126 else f\"\\\\x{actual_byte:02x}\"\n",
        "\n",
        "        print(f\"{i}: '{byte_char}' (byte {actual_byte}) -> \", end=\"\")\n",
        "\n",
        "        for j in range(top_k):\n",
        "            pred_byte = top_indices[i, j].item()\n",
        "            pred_prob = top_probs[i, j].item()\n",
        "            pred_char = chr(pred_byte) if 32 <= pred_byte <= 126 else f\"\\\\x{pred_byte:02x}\"\n",
        "\n",
        "            print(f\"{pred_char} ({pred_byte}): {pred_prob:.4f}\", end=\"\")\n",
        "            if j < top_k - 1:\n",
        "                print(\", \", end=\"\")\n",
        "\n",
        "        print()"
      ],
      "metadata": {
        "id": "-PnJejUPo8K2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Complete Usage Example with Google Colab**\n",
        "\n",
        "Here's a complete example of how to use our Byte Latent Transformer in Google Colab:"
      ],
      "metadata": {
        "id": "MBdyH7Elo_hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colab_main():\n",
        "    \"\"\"Main function for Google Colab usage.\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import torch\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Menu for user options\n",
        "    while True:\n",
        "        print(\"\\n=== Byte Latent Transformer ===\")\n",
        "        print(\"1. Train a new model\")\n",
        "        print(\"2. Load a trained model\")\n",
        "        print(\"3. Generate text\")\n",
        "        print(\"4. Interactive text generation\")\n",
        "        print(\"5. Evaluate model\")\n",
        "        print(\"6. Upload/download model\")\n",
        "        print(\"7. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-7): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            # Train a new model\n",
        "            print(\"\\n=== Training a New Model ===\")\n",
        "\n",
        "            # Ask user to upload a text file\n",
        "            print(\"Please upload a text file for training:\")\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if not uploaded:\n",
        "                print(\"No file uploaded. Returning to menu.\")\n",
        "                continue\n",
        "\n",
        "            file_name = list(uploaded.keys())[0]\n",
        "            file_path = file_name\n",
        "\n",
        "            # Get training parameters\n",
        "            try:\n",
        "                hidden_dim = int(input(\"Hidden dimension (default 256): \") or 256)\n",
        "                latent_dim = int(input(\"Latent dimension (default 128): \") or 128)\n",
        "                num_layers = int(input(\"Number of layers (default 4): \") or 4)\n",
        "                num_heads = int(input(\"Number of attention heads (default 4): \") or 4)\n",
        "                epochs = int(input(\"Number of epochs (default 5): \") or 5)\n",
        "                batch_size = int(input(\"Batch size (default 8): \") or 8)\n",
        "                max_seq_length = int(input(\"Maximum sequence length (default 128): \") or 128)\n",
        "\n",
        "                # Import functions from Part 2\n",
        "                from Part2_ByteLatentTransformer_Training import train_byte_latent_transformer\n",
        "\n",
        "                # Train the model\n",
        "                model, monitor = train_byte_latent_transformer(\n",
        "                    file_path=file_path,\n",
        "                    hidden_dim=hidden_dim,\n",
        "                    latent_dim=latent_dim,\n",
        "                    num_layers=num_layers,\n",
        "                    num_heads=num_heads,\n",
        "                    ff_dim=hidden_dim * 4,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                # Save the trained model\n",
        "                save_path = f\"byte_latent_transformer_{hidden_dim}_{latent_dim}_{num_layers}.pt\"\n",
        "                save_model(model, path=save_path)\n",
        "\n",
        "                # Download the trained model\n",
        "                files.download(save_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training: {e}\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            # Load a trained model\n",
        "            print(\"\\n=== Loading a Trained Model ===\")\n",
        "            print(\"Please upload a trained model file (.pt):\")\n",
        "\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if not uploaded:\n",
        "                print(\"No file uploaded. Returning to menu.\")\n",
        "                continue\n",
        "\n",
        "            model_file = list(uploaded.keys())[0]\n",
        "\n",
        "            try:\n",
        "                model, checkpoint = load_model(model_file, device=device)\n",
        "                print(\"Model loaded successfully!\")\n",
        "\n",
        "                # Display model information\n",
        "                config = checkpoint.get('model_config', {})\n",
        "                print(f\"Model configuration:\")\n",
        "                print(f\"- Hidden dimension: {config.get('hidden_dim', 'Unknown')}\")\n",
        "                print(f\"- Latent dimension: {config.get('latent_dim', 'Unknown')}\")\n",
        "                print(f\"- Number of layers: {config.get('num_layers', 'Unknown')}\")\n",
        "                print(f\"- Number of attention heads: {config.get('num_heads', 'Unknown')}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model: {e}\")\n",
        "                model = None\n",
        "\n",
        "        elif choice == '3':\n",
        "            # Generate text\n",
        "            if not locals().get('model'):\n",
        "                print(\"No model loaded. Please load a model first.\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\n=== Generate Text ===\")\n",
        "            prompt = input(\"Enter a prompt: \")\n",
        "\n",
        "            try:\n",
        "                temperature = float(input(\"Temperature (0.1-1.5, default 0.8): \") or 0.8)\n",
        "                max_length = int(input(\"Max length to generate (default 100): \") or 100)\n",
        "                top_k = int(input(\"Top-k value (default 50, 0 to disable): \") or 50)\n",
        "                top_p = float(input(\"Top-p value (default 0.9, 0 to disable): \") or 0.9)\n",
        "\n",
        "                # Generate text\n",
        "                generated_text = generate_text(\n",
        "                    model=model,\n",
        "                    prompt_bytes=prompt,\n",
        "                    max_length=max_length,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k if top_k > 0 else None,\n",
        "                    top_p=top_p if top_p > 0 else None,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                print(\"\\n=== Generated Text ===\")\n",
        "                print(generated_text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during text generation: {e}\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # Interactive text generation\n",
        "            if not locals().get('model'):\n",
        "                print(\"No model loaded. Please load a model first.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                interactive_generation(model, device=device)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during interactive generation: {e}\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            # Evaluate model\n",
        "            if not locals().get('model'):\n",
        "                print(\"No model loaded. Please load a model first.\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\n=== Evaluate Model ===\")\n",
        "            print(\"1. Measure inference speed\")\n",
        "            print(\"2. Analyze byte distribution\")\n",
        "            print(\"3. Return to main menu\")\n",
        "\n",
        "            eval_choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "            if eval_choice == '1':\n",
        "                # Measure inference speed\n",
        "                try:\n",
        "                    batch_size = int(input(\"Batch size (default 1): \") or 1)\n",
        "                    seq_length = int(input(\"Sequence length (default 128): \") or 128)\n",
        "                    num_iterations = int(input(\"Number of iterations (default 10): \") or 10)\n",
        "\n",
        "                    results = evaluate_model_speed(\n",
        "                        model=model,\n",
        "                        seq_length=seq_length,\n",
        "                        batch_size=batch_size,\n",
        "                        num_iterations=num_iterations,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    print(\"\\n=== Performance Results ===\")\n",
        "                    print(f\"Total time: {results['total_time']:.4f} seconds\")\n",
        "                    print(f\"Average time per batch: {results['avg_time_per_batch'] * 1000:.4f} ms\")\n",
        "                    print(f\"Tokens per second: {results['tokens_per_second']:.2f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during performance evaluation: {e}\")\n",
        "\n",
        "            elif eval_choice == '2':\n",
        "                # Analyze byte distribution\n",
        "                try:\n",
        "                    text = input(\"Enter text to analyze: \")\n",
        "                    analyze_byte_distribution(model, text, device=device)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during byte distribution analysis: {e}\")\n",
        "\n",
        "        elif choice == '6':\n",
        "            # Upload/download model\n",
        "            print(\"\\n=== Upload/Download Model ===\")\n",
        "            print(\"1. Upload model\")\n",
        "            print(\"2. Download current model\")\n",
        "            print(\"3. Return to main menu\")\n",
        "\n",
        "            file_choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "            if file_choice == '1':\n",
        "                # Upload model\n",
        "                print(\"Please upload a trained model file (.pt):\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                if not uploaded:\n",
        "                    print(\"No file uploaded. Returning to menu.\")\n",
        "                    continue\n",
        "\n",
        "                model_file = list(uploaded.keys())[0]\n",
        "\n",
        "                try:\n",
        "                    model, checkpoint = load_model(model_file, device=device)\n",
        "                    print(\"Model loaded successfully!\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading model: {e}\")\n",
        "\n",
        "            elif file_choice == '2':\n",
        "                # Download current model\n",
        "                if not locals().get('model'):\n",
        "                    print(\"No model loaded. Please load a model first.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    save_path = input(\"Enter filename to save as (default: model.pt): \") or \"model.pt\"\n",
        "                    save_model(model, path=save_path)\n",
        "                    files.download(save_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading model: {e}\")\n",
        "\n",
        "        elif choice == '7':\n",
        "            # Exit\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "    except ImportError:\n",
        "        is_colab = False\n",
        "\n",
        "    if is_colab:\n",
        "        colab_main()\n",
        "    else:\n",
        "        # When running locally, use a simpler interface\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        model = None\n",
        "        prompt = \"Hello, world!\"\n",
        "\n",
        "        while True:\n",
        "            print(\"\\n=== Byte Latent Transformer ===\")\n",
        "            print(\"1. Train a new model\")\n",
        "            print(\"2. Load a trained model\")\n",
        "            print(\"3. Generate text\")\n",
        "            print(\"4. Exit\")\n",
        "\n",
        "            choice = input(\"\\nEnter your choice (1-4): \")\n",
        "\n",
        "            # Implement local options similar to the Colab interface\n",
        "            if choice == '4':\n",
        "                break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BobjcSgtpDVP",
        "outputId": "84fd1ee3-3260-4434-a124-fd2e8d29f0d6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "=== Byte Latent Transformer ===\n",
            "1. Train a new model\n",
            "2. Load a trained model\n",
            "3. Generate text\n",
            "4. Interactive text generation\n",
            "5. Evaluate model\n",
            "6. Upload/download model\n",
            "7. Exit\n",
            "\n",
            "Enter your choice (1-7): 7\n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the implementation of our Byte Latent Transformer. You now have a fully functional model that can:\n",
        "\n",
        "1.Process text data at the byte level\n",
        "\n",
        "3.Train efficiently with a latent space projection\n",
        "\n",
        "4.Generate text with various sampling strategies\n",
        "\n",
        "5.Evaluate model performance and analyze byte distributions\n",
        "\n",
        "The implementation is organized into three parts for better understanding and modularity, and comes with a Google Colab interface for easy usage."
      ],
      "metadata": {
        "id": "opxj-WF_pYdZ"
      }
    }
  ]
}